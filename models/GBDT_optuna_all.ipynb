{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optuna hyperparameter tuning: LightGBM, XGBoost, CatBoost (GPU)\n",
        "\n",
        "End-to-end: load/clean, tune each model with Optuna (with pruning where supported), retrain best, evaluate, and save timestamped artifacts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: (1492, 2076)\n",
            "Dropped NaN labels: 1\n",
            "Train/Test: (1043, 2073) (448, 2073)\n"
          ]
        }
      ],
      "source": [
        "import os, json, warnings, datetime, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DATA_PATH = 'dataset_pruned.csv'\n",
        "LABEL_COL = 'weight_kg'\n",
        "ID_COLS = ['Chicken_ID', 'Image_ID']\n",
        "\n",
        "assert os.path.exists(DATA_PATH), f\"Missing {DATA_PATH}\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print('Loaded:', df.shape)\n",
        "\n",
        "before = df.shape[0]\n",
        "df = df[~df[LABEL_COL].isna()].reset_index(drop=True)\n",
        "print('Dropped NaN labels:', before - df.shape[0])\n",
        "\n",
        "feature_cols = [c for c in df.columns if c not in (ID_COLS + [LABEL_COL])]\n",
        "for c in feature_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "X = df[feature_cols]\n",
        "y = df[LABEL_COL].values\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_tr_imp = imputer.fit_transform(X_tr)\n",
        "X_te_imp = imputer.transform(X_te)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_train = scaler.fit_transform(X_tr_imp)\n",
        "X_test = scaler.transform(X_te_imp)\n",
        "\n",
        "print('Train/Test:', X_train.shape, X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna: 4.4.0\n"
          ]
        }
      ],
      "source": [
        "# Install/import Optuna and integrations\n",
        "# %pip install optuna optuna-integration[lightgbm]\n",
        "import optuna\n",
        "from optuna_integration import LightGBMPruningCallback\n",
        "print('Optuna:', optuna.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-13 08:21:14,212] A new study created in memory with name: no-name-c66315bd-4d8d-4744-b3a2-2229520b09f6\n",
            "[I 2025-08-13 08:21:16,311] Trial 0 finished with value: 0.07278520875282395 and parameters: {'n_estimators': 3185, 'learning_rate': 0.13125830316209655, 'num_leaves': 97, 'max_depth': 7, 'min_child_samples': 10, 'min_child_weight': 0.004207053950287938, 'subsample': 0.6232334448672797, 'colsample_bytree': 0.9464704583099741, 'reg_alpha': 1.2022300234864176, 'reg_lambda': 3.6863266000822046}. Best is trial 0 with value: 0.07278520875282395.\n",
            "[I 2025-08-13 08:21:18,791] Trial 1 finished with value: 0.09739247636399265 and parameters: {'n_estimators': 1592, 'learning_rate': 0.13826189316223852, 'num_leaves': 109, 'max_depth': 1, 'min_child_samples': 11, 'min_child_weight': 0.00541524411940254, 'subsample': 0.7216968971838151, 'colsample_bytree': 0.8099025726528951, 'reg_alpha': 0.8638900372842315, 'reg_lambda': 1.8105311308911887}. Best is trial 0 with value: 0.07278520875282395.\n",
            "[I 2025-08-13 08:21:28,348] Trial 2 finished with value: 0.07498476503213418 and parameters: {'n_estimators': 4253, 'learning_rate': 0.01459007452373112, 'num_leaves': 48, 'max_depth': 4, 'min_child_samples': 21, 'min_child_weight': 1.382623217936987, 'subsample': 0.6798695128633439, 'colsample_bytree': 0.8056937753654446, 'reg_alpha': 1.184829137724085, 'reg_lambda': 0.7090268572399898}. Best is trial 0 with value: 0.07278520875282395.\n",
            "[I 2025-08-13 08:21:37,045] Trial 3 finished with value: 0.07334041292556885 and parameters: {'n_estimators': 4234, 'learning_rate': 0.015869086642715014, 'num_leaves': 22, 'max_depth': 12, 'min_child_samples': 39, 'min_child_weight': 1.7123375973163988, 'subsample': 0.7218455076693483, 'colsample_bytree': 0.6390688456025535, 'reg_alpha': 1.3684660530243138, 'reg_lambda': 2.480686221828206}. Best is trial 0 with value: 0.07278520875282395.\n",
            "[I 2025-08-13 08:21:41,701] Trial 4 finished with value: 0.06947867917469104 and parameters: {'n_estimators': 2049, 'learning_rate': 0.03822726574649208, 'num_leaves': 18, 'max_depth': 11, 'min_child_samples': 14, 'min_child_weight': 0.4467752817973907, 'subsample': 0.7246844304357644, 'colsample_bytree': 0.8080272084711243, 'reg_alpha': 1.0934205586865593, 'reg_lambda': 1.3318450498648717}. Best is trial 4 with value: 0.06947867917469104.\n",
            "[I 2025-08-13 08:21:50,000] Trial 5 finished with value: 0.06667755972863389 and parameters: {'n_estimators': 5864, 'learning_rate': 0.08158812228791566, 'num_leaves': 121, 'max_depth': 11, 'min_child_samples': 26, 'min_child_weight': 4.869640941520899, 'subsample': 0.6353970008207678, 'colsample_bytree': 0.6783931449676581, 'reg_alpha': 0.09045457782107613, 'reg_lambda': 1.9639864884346896}. Best is trial 5 with value: 0.06667755972863389.\n",
            "[I 2025-08-13 08:22:14,821] Trial 6 finished with value: 0.0747864712822062 and parameters: {'n_estimators': 3249, 'learning_rate': 0.02085120818436357, 'num_leaves': 108, 'max_depth': 3, 'min_child_samples': 15, 'min_child_weight': 0.14817820606039092, 'subsample': 0.6563696899899051, 'colsample_bytree': 0.9208787923016158, 'reg_alpha': 0.14910128735954165, 'reg_lambda': 4.940991214702327}. Best is trial 5 with value: 0.06667755972863389.\n",
            "[I 2025-08-13 08:22:26,897] Trial 7 finished with value: 0.0710848656485853 and parameters: {'n_estimators': 4975, 'learning_rate': 0.017128044242493735, 'num_leaves': 15, 'max_depth': 10, 'min_child_samples': 30, 'min_child_weight': 0.8241925264876453, 'subsample': 0.9085081386743783, 'colsample_bytree': 0.6296178606936361, 'reg_alpha': 0.7169314570885452, 'reg_lambda': 1.0214107678630837}. Best is trial 5 with value: 0.06667755972863389.\n",
            "[I 2025-08-13 08:22:29,723] Trial 8 finished with value: 0.07485542134434717 and parameters: {'n_estimators': 5384, 'learning_rate': 0.054082340576877566, 'num_leaves': 52, 'max_depth': -1, 'min_child_samples': 16, 'min_child_weight': 0.01998634077852888, 'subsample': 0.8918424713352255, 'colsample_bytree': 0.8550229885420852, 'reg_alpha': 1.774425485152653, 'reg_lambda': 2.6249671632287717}. Best is trial 5 with value: 0.06667755972863389.\n",
            "[I 2025-08-13 08:22:40,485] Trial 9 finished with value: 0.06882014198221502 and parameters: {'n_estimators': 2038, 'learning_rate': 0.06899870818520017, 'num_leaves': 100, 'max_depth': 6, 'min_child_samples': 32, 'min_child_weight': 0.09444574254983566, 'subsample': 0.8090931317527976, 'colsample_bytree': 0.7710164073434198, 'reg_alpha': 0.05083825348819038, 'reg_lambda': 0.98551142146987}. Best is trial 5 with value: 0.06667755972863389.\n",
            "[I 2025-08-13 08:22:45,064] Trial 10 finished with value: 0.06680432634955007 and parameters: {'n_estimators': 5908, 'learning_rate': 0.07903507924322126, 'num_leaves': 124, 'max_depth': 8, 'min_child_samples': 27, 'min_child_weight': 4.2722748162268, 'subsample': 0.983908298378168, 'colsample_bytree': 0.7140319566048057, 'reg_alpha': 0.4249397448399991, 'reg_lambda': 3.6646636795759484}. Best is trial 5 with value: 0.06667755972863389.\n",
            "[I 2025-08-13 08:22:49,742] Trial 11 finished with value: 0.06721528657409213 and parameters: {'n_estimators': 5981, 'learning_rate': 0.08171759864501184, 'num_leaves': 124, 'max_depth': 8, 'min_child_samples': 26, 'min_child_weight': 6.8999689896843694, 'subsample': 0.996380128364447, 'colsample_bytree': 0.7001401322972585, 'reg_alpha': 0.4461068357921353, 'reg_lambda': 3.6921480746020947}. Best is trial 5 with value: 0.06667755972863389.\n",
            "[I 2025-08-13 08:22:50,185] Trial 12 pruned. Trial was pruned at iteration 0.\n",
            "[I 2025-08-13 08:22:54,210] Trial 13 finished with value: 0.06851103285495594 and parameters: {'n_estimators': 5027, 'learning_rate': 0.09203803265148959, 'num_leaves': 81, 'max_depth': 12, 'min_child_samples': 35, 'min_child_weight': 3.2982176319651146, 'subsample': 0.9903570651984263, 'colsample_bytree': 0.7213785203554232, 'reg_alpha': 0.4118479169074303, 'reg_lambda': 4.458262670269306}. Best is trial 5 with value: 0.06667755972863389.\n",
            "[I 2025-08-13 08:22:54,566] Trial 14 pruned. Trial was pruned at iteration 0.\n",
            "[I 2025-08-13 08:22:57,513] Trial 15 pruned. Trial was pruned at iteration 371.\n",
            "[I 2025-08-13 08:22:57,936] Trial 16 pruned. Trial was pruned at iteration 0.\n",
            "[I 2025-08-13 08:22:58,338] Trial 17 pruned. Trial was pruned at iteration 0.\n",
            "[I 2025-08-13 08:22:59,505] Trial 18 pruned. Trial was pruned at iteration 67.\n",
            "[I 2025-08-13 08:23:06,645] Trial 19 finished with value: 0.0660321802074292 and parameters: {'n_estimators': 4591, 'learning_rate': 0.07267854842624646, 'num_leaves': 127, 'max_depth': 11, 'min_child_samples': 29, 'min_child_weight': 0.03895852682376525, 'subsample': 0.9464322236819168, 'colsample_bytree': 0.9925252347033174, 'reg_alpha': 0.2542876840177446, 'reg_lambda': 1.6202515269885749}. Best is trial 19 with value: 0.0660321802074292.\n",
            "[I 2025-08-13 08:23:06,989] Trial 20 pruned. Trial was pruned at iteration 0.\n",
            "[I 2025-08-13 08:23:07,414] Trial 21 pruned. Trial was pruned at iteration 0.\n",
            "[I 2025-08-13 08:23:13,547] Trial 22 finished with value: 0.06650877753932936 and parameters: {'n_estimators': 5681, 'learning_rate': 0.07344812263250865, 'num_leaves': 119, 'max_depth': 12, 'min_child_samples': 23, 'min_child_weight': 2.7408426692987007, 'subsample': 0.9454657053741977, 'colsample_bytree': 0.9997653976350742, 'reg_alpha': 0.34974465467776944, 'reg_lambda': 1.484679990306998}. Best is trial 19 with value: 0.0660321802074292.\n",
            "[I 2025-08-13 08:23:13,930] Trial 23 pruned. Trial was pruned at iteration 0.\n",
            "[I 2025-08-13 08:23:14,361] Trial 24 pruned. Trial was pruned at iteration 0.\n",
            "[I 2025-08-13 08:23:17,320] Trial 25 finished with value: 0.0673135589018723 and parameters: {'n_estimators': 5567, 'learning_rate': 0.11418667775886775, 'num_leaves': 118, 'max_depth': 12, 'min_child_samples': 19, 'min_child_weight': 1.7243501671927375, 'subsample': 0.9356725765600427, 'colsample_bytree': 0.9557463446872989, 'reg_alpha': 0.552234033056158, 'reg_lambda': 0.6515345217881958}. Best is trial 19 with value: 0.0660321802074292.\n",
            "[I 2025-08-13 08:23:17,690] Trial 26 pruned. Trial was pruned at iteration 0.\n",
            "[I 2025-08-13 08:23:18,118] Trial 27 pruned. Trial was pruned at iteration 0.\n",
            "[I 2025-08-13 08:23:29,157] Trial 28 finished with value: 0.06522275373124217 and parameters: {'n_estimators': 4806, 'learning_rate': 0.09070338356064767, 'num_leaves': 85, 'max_depth': 9, 'min_child_samples': 20, 'min_child_weight': 0.04632441253248764, 'subsample': 0.9642868346743416, 'colsample_bytree': 0.9171698630809318, 'reg_alpha': 0.027033333661296344, 'reg_lambda': 2.9332111560418825}. Best is trial 28 with value: 0.06522275373124217.\n",
            "[I 2025-08-13 08:23:30,873] Trial 29 pruned. Trial was pruned at iteration 86.\n",
            "[I 2025-08-13 08:23:44,067] Trial 30 finished with value: 0.06392461865330033 and parameters: {'n_estimators': 4765, 'learning_rate': 0.11830121503750618, 'num_leaves': 103, 'max_depth': 6, 'min_child_samples': 12, 'min_child_weight': 0.002389958468515018, 'subsample': 0.9741745208685237, 'colsample_bytree': 0.9744197657087739, 'reg_alpha': 0.004813903304348999, 'reg_lambda': 2.523675000757692}. Best is trial 30 with value: 0.06392461865330033.\n",
            "[I 2025-08-13 08:23:53,712] Trial 31 finished with value: 0.0659581069909849 and parameters: {'n_estimators': 4735, 'learning_rate': 0.12046553267535294, 'num_leaves': 102, 'max_depth': 6, 'min_child_samples': 6, 'min_child_weight': 0.0010315017149009943, 'subsample': 0.9679161264008204, 'colsample_bytree': 0.9721753161770135, 'reg_alpha': 0.01180160490644383, 'reg_lambda': 2.809413356457547}. Best is trial 30 with value: 0.06392461865330033.\n",
            "[I 2025-08-13 08:24:00,514] Trial 32 pruned. Trial was pruned at iteration 371.\n",
            "[I 2025-08-13 08:24:01,352] Trial 33 pruned. Trial was pruned at iteration 28.\n",
            "[I 2025-08-13 08:24:01,767] Trial 34 pruned. Trial was pruned at iteration 3.\n",
            "[I 2025-08-13 08:24:12,110] Trial 35 finished with value: 0.06293188512887113 and parameters: {'n_estimators': 4722, 'learning_rate': 0.13531172137826583, 'num_leaves': 89, 'max_depth': 7, 'min_child_samples': 8, 'min_child_weight': 0.002518217318083091, 'subsample': 0.8913612491094716, 'colsample_bytree': 0.93677767183079, 'reg_alpha': 0.02184720353025197, 'reg_lambda': 2.602733744113}. Best is trial 35 with value: 0.06293188512887113.\n",
            "[I 2025-08-13 08:24:18,399] Trial 36 pruned. Trial was pruned at iteration 371.\n",
            "[I 2025-08-13 08:24:19,447] Trial 37 pruned. Trial was pruned at iteration 33.\n",
            "[I 2025-08-13 08:24:20,201] Trial 38 pruned. Trial was pruned at iteration 29.\n",
            "[I 2025-08-13 08:24:25,962] Trial 39 finished with value: 0.06497400974743618 and parameters: {'n_estimators': 5243, 'learning_rate': 0.08996926292094486, 'num_leaves': 63, 'max_depth': 7, 'min_child_samples': 8, 'min_child_weight': 0.006558188086461593, 'subsample': 0.9994292852334551, 'colsample_bytree': 0.8382359957954869, 'reg_alpha': 0.13266020773287274, 'reg_lambda': 2.7676010726689992}. Best is trial 35 with value: 0.06293188512887113.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LGBM best MAE: 0.06293188512887113\n"
          ]
        }
      ],
      "source": [
        "# LightGBM study\n",
        "import lightgbm as lgb\n",
        "\n",
        "def lgb_objective(trial):\n",
        "    params = dict(\n",
        "        n_estimators=trial.suggest_int('n_estimators', 1500, 6000),\n",
        "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
        "        num_leaves=trial.suggest_int('num_leaves', 15, 127),\n",
        "        max_depth=trial.suggest_int('max_depth', -1, 12),\n",
        "        min_child_samples=trial.suggest_int('min_child_samples', 5, 40),\n",
        "        min_child_weight=trial.suggest_float('min_child_weight', 1e-3, 10.0, log=True),\n",
        "        subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        reg_alpha=trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
        "        reg_lambda=trial.suggest_float('reg_lambda', 0.5, 5.0),\n",
        "        objective='regression', n_jobs=-1, verbosity=-1,\n",
        "        device='gpu', gpu_platform_id=0, gpu_device_id=0,\n",
        "    )\n",
        "    model = lgb.LGBMRegressor(**params)\n",
        "    model.fit(\n",
        "        X_train, y_tr,\n",
        "        eval_set=[(X_test, y_te)], eval_metric='l1',\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=300, verbose=False),\n",
        "            lgb.log_evaluation(period=0),\n",
        "            LightGBMPruningCallback(trial, 'l1'),\n",
        "        ],\n",
        "    )\n",
        "    pred = model.predict(X_test)\n",
        "    return mean_absolute_error(y_te, pred)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "study_lgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42),\n",
        "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=10))\n",
        "study_lgb.optimize(lgb_objective, n_trials=40, n_jobs=1)\n",
        "print('LGBM best MAE:', study_lgb.best_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-13 08:24:26,002] A new study created in memory with name: no-name-ffc72c45-c9f6-46a8-85e3-aaad7578f33c\n",
            "[I 2025-08-13 08:24:51,403] Trial 0 finished with value: 0.08127061954779283 and parameters: {'n_estimators': 3185, 'learning_rate': 0.17254716573280354, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.6624074561769746, 'colsample_bytree': 0.662397808134481, 'reg_alpha': 0.11616722433639892, 'reg_lambda': 4.3977926559872085}. Best is trial 0 with value: 0.08127061954779283.\n",
            "[I 2025-08-13 08:25:07,641] Trial 1 finished with value: 0.07506177873483726 and parameters: {'n_estimators': 4205, 'learning_rate': 0.08341106432362087, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.9329770563201687, 'colsample_bytree': 0.6849356442713105, 'reg_alpha': 0.36364993441420124, 'reg_lambda': 1.325320294340452}. Best is trial 1 with value: 0.07506177873483726.\n",
            "[I 2025-08-13 08:25:29,120] Trial 2 finished with value: 0.0714482939637133 and parameters: {'n_estimators': 2869, 'learning_rate': 0.048164145309070844, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.8447411578889518, 'colsample_bytree': 0.6557975442608167, 'reg_alpha': 0.5842892970704363, 'reg_lambda': 2.148628294821613}. Best is trial 2 with value: 0.0714482939637133.\n",
            "[I 2025-08-13 08:25:44,407] Trial 3 finished with value: 0.0763394109872835 and parameters: {'n_estimators': 3552, 'learning_rate': 0.10508421338691762, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.836965827544817, 'colsample_bytree': 0.6185801650879991, 'reg_alpha': 1.2150897038028767, 'reg_lambda': 1.2673585565928118}. Best is trial 2 with value: 0.0714482939637133.\n",
            "[I 2025-08-13 08:25:56,976] Trial 4 finished with value: 0.08141773578098842 and parameters: {'n_estimators': 1792, 'learning_rate': 0.17160445029754812, 'max_depth': 10, 'min_child_weight': 7, 'subsample': 0.7218455076693483, 'colsample_bytree': 0.6390688456025535, 'reg_alpha': 1.3684660530243138, 'reg_lambda': 2.480686221828206}. Best is trial 2 with value: 0.0714482939637133.\n",
            "[I 2025-08-13 08:26:13,226] Trial 5 finished with value: 0.07477228237582104 and parameters: {'n_estimators': 2049, 'learning_rate': 0.04407984038169244, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.7035119926400067, 'colsample_bytree': 0.8650089137415928, 'reg_alpha': 0.6234221521788219, 'reg_lambda': 2.8403060953001487}. Best is trial 2 with value: 0.0714482939637133.\n",
            "[I 2025-08-13 08:27:04,908] Trial 6 finished with value: 0.07191242592675347 and parameters: {'n_estimators': 3960, 'learning_rate': 0.017398074711291726, 'max_depth': 10, 'min_child_weight': 7, 'subsample': 0.9757995766256756, 'colsample_bytree': 0.9579309401710595, 'reg_alpha': 1.1957999576221703, 'reg_lambda': 4.6484340576040255}. Best is trial 2 with value: 0.0714482939637133.\n",
            "[I 2025-08-13 08:27:21,221] Trial 7 finished with value: 0.07904042733992848 and parameters: {'n_estimators': 1898, 'learning_rate': 0.017987863473362915, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.7554709158757928, 'colsample_bytree': 0.7085396127095583, 'reg_alpha': 1.6574750183038587, 'reg_lambda': 2.105389970121152}. Best is trial 2 with value: 0.0714482939637133.\n",
            "[I 2025-08-13 08:27:41,505] Trial 8 finished with value: 0.07716736880689859 and parameters: {'n_estimators': 2764, 'learning_rate': 0.05082341959721458, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.6298202574719083, 'colsample_bytree': 0.9947547746402069, 'reg_alpha': 1.5444895385933148, 'reg_lambda': 1.3942205669037757}. Best is trial 2 with value: 0.0714482939637133.\n",
            "[I 2025-08-13 08:27:53,714] Trial 9 finished with value: 0.07533800203246731 and parameters: {'n_estimators': 1524, 'learning_rate': 0.11506408247250169, 'max_depth': 8, 'min_child_weight': 6, 'subsample': 0.9085081386743783, 'colsample_bytree': 0.6296178606936361, 'reg_alpha': 0.7169314570885452, 'reg_lambda': 1.0214107678630837}. Best is trial 2 with value: 0.0714482939637133.\n",
            "[I 2025-08-13 08:28:49,744] Trial 10 finished with value: 0.0745019987885441 and parameters: {'n_estimators': 5398, 'learning_rate': 0.010181283131439633, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 0.8277250010609204, 'colsample_bytree': 0.777231918720281, 'reg_alpha': 1.9195414918908404, 'reg_lambda': 3.500862237871602}. Best is trial 2 with value: 0.0714482939637133.\n",
            "[I 2025-08-13 08:29:26,041] Trial 11 finished with value: 0.07082706421507255 and parameters: {'n_estimators': 4405, 'learning_rate': 0.02525611096184472, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.9850231831978604, 'colsample_bytree': 0.9499856595509166, 'reg_alpha': 0.9454280133158974, 'reg_lambda': 4.749245545228564}. Best is trial 11 with value: 0.07082706421507255.\n",
            "[I 2025-08-13 08:30:02,569] Trial 12 finished with value: 0.06905144540326937 and parameters: {'n_estimators': 4932, 'learning_rate': 0.037243422013756265, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.883430193954917, 'colsample_bytree': 0.8758146404482569, 'reg_alpha': 0.8538844743864877, 'reg_lambda': 3.588473880816479}. Best is trial 12 with value: 0.06905144540326937.\n",
            "[I 2025-08-13 08:30:45,757] Trial 13 finished with value: 0.069643358135862 and parameters: {'n_estimators': 4974, 'learning_rate': 0.0301510303963567, 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.9956145525799184, 'colsample_bytree': 0.8938825194808143, 'reg_alpha': 0.8762603349822689, 'reg_lambda': 3.9954446685081537}. Best is trial 12 with value: 0.06905144540326937.\n",
            "[I 2025-08-13 08:31:39,134] Trial 14 finished with value: 0.07108922414162329 and parameters: {'n_estimators': 5587, 'learning_rate': 0.029391456795476657, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.8987453277869946, 'colsample_bytree': 0.8884404511750978, 'reg_alpha': 0.9242295168022562, 'reg_lambda': 3.8214425395680474}. Best is trial 12 with value: 0.06905144540326937.\n",
            "[I 2025-08-13 08:32:30,326] Trial 15 finished with value: 0.06808339164831809 and parameters: {'n_estimators': 4923, 'learning_rate': 0.0325913314552552, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.8819708330406043, 'colsample_bytree': 0.8506773182380161, 'reg_alpha': 0.3554240625075822, 'reg_lambda': 3.6896969073505725}. Best is trial 15 with value: 0.06808339164831809.\n",
            "[I 2025-08-13 08:33:03,735] Trial 16 finished with value: 0.06916263592136758 and parameters: {'n_estimators': 4935, 'learning_rate': 0.0681107800180524, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.7932711244035222, 'colsample_bytree': 0.7993463673379999, 'reg_alpha': 0.009480239306178584, 'reg_lambda': 3.2079337585064462}. Best is trial 15 with value: 0.06808339164831809.\n",
            "[I 2025-08-13 08:33:58,082] Trial 17 finished with value: 0.06705802364434514 and parameters: {'n_estimators': 5909, 'learning_rate': 0.037077763919231385, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.8733689916156429, 'colsample_bytree': 0.8339918701645398, 'reg_alpha': 0.3019957087978521, 'reg_lambda': 3.17848012563149}. Best is trial 17 with value: 0.06705802364434514.\n",
            "[I 2025-08-13 08:35:23,436] Trial 18 finished with value: 0.0689360737428069 and parameters: {'n_estimators': 5919, 'learning_rate': 0.021512749160028457, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.7891882375054343, 'colsample_bytree': 0.7498828113568031, 'reg_alpha': 0.38463800764305583, 'reg_lambda': 2.995800029352226}. Best is trial 17 with value: 0.06705802364434514.\n",
            "[I 2025-08-13 08:36:54,210] Trial 19 finished with value: 0.06731778260001114 and parameters: {'n_estimators': 5996, 'learning_rate': 0.012221521636526499, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.9363179607010957, 'colsample_bytree': 0.8366070683982902, 'reg_alpha': 0.3150533628362826, 'reg_lambda': 4.170784267892943}. Best is trial 17 with value: 0.06705802364434514.\n",
            "[I 2025-08-13 08:39:00,150] Trial 20 finished with value: 0.06513051303476095 and parameters: {'n_estimators': 5941, 'learning_rate': 0.010102504092741684, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.9433652849644489, 'colsample_bytree': 0.8313878719050191, 'reg_alpha': 0.20175845967196837, 'reg_lambda': 4.257076008836355}. Best is trial 20 with value: 0.06513051303476095.\n",
            "[I 2025-08-13 08:41:04,416] Trial 21 finished with value: 0.06525007656429495 and parameters: {'n_estimators': 5998, 'learning_rate': 0.01062997828284409, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.9452887704826716, 'colsample_bytree': 0.8291311937961049, 'reg_alpha': 0.19125182730314527, 'reg_lambda': 4.273641800176621}. Best is trial 20 with value: 0.06513051303476095.\n",
            "[I 2025-08-13 08:42:47,985] Trial 22 finished with value: 0.06633080982416868 and parameters: {'n_estimators': 5398, 'learning_rate': 0.014236948702908885, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.9276629795475456, 'colsample_bytree': 0.8072011317670672, 'reg_alpha': 0.16590460441804328, 'reg_lambda': 4.9423206976368945}. Best is trial 20 with value: 0.06513051303476095.\n",
            "[I 2025-08-13 08:44:29,467] Trial 23 finished with value: 0.06765393880222524 and parameters: {'n_estimators': 5431, 'learning_rate': 0.014685947913087439, 'max_depth': 9, 'min_child_weight': 5, 'subsample': 0.9449923596722282, 'colsample_bytree': 0.7416233253973241, 'reg_alpha': 0.11788401836301077, 'reg_lambda': 4.952575649894518}. Best is trial 20 with value: 0.06513051303476095.\n",
            "[W 2025-08-13 08:46:19,806] Trial 24 failed with parameters: {'n_estimators': 5297, 'learning_rate': 0.010531842861291696, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.9600692058568724, 'colsample_bytree': 0.800030192808931, 'reg_alpha': 0.1589398201856389, 'reg_lambda': 4.37909365195587} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"d:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"C:\\Temp\\ipykernel_12836\\1805549864.py\", line 18, in xgb_objective\n",
            "    model.fit(X_train, y_tr, eval_set=[(X_test, y_te)], verbose=False)\n",
            "  File \"d:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"d:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
            "    self._Booster = train(\n",
            "                    ^^^^^^\n",
            "  File \"d:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"d:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
            "    bst.update(dtrain, iteration=i, fobj=obj)\n",
            "  File \"d:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\xgboost\\core.py\", line 2247, in update\n",
            "    _LIB.XGBoosterUpdateOneIter(\n",
            "KeyboardInterrupt\n",
            "[W 2025-08-13 08:46:19,809] Trial 24 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_absolute_error(y_te, pred)\n\u001b[32m     22\u001b[39m study_xgb = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m, sampler=optuna.samplers.TPESampler(seed=\u001b[32m42\u001b[39m),\n\u001b[32m     23\u001b[39m                                 pruner=optuna.pruners.MedianPruner(n_startup_trials=\u001b[32m10\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mstudy_xgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgb_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mXGB best MAE:\u001b[39m\u001b[33m'\u001b[39m, study_xgb.best_value)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mxgb_objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m      5\u001b[39m params = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m      6\u001b[39m     n_estimators=trial.suggest_int(\u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1500\u001b[39m, \u001b[32m6000\u001b[39m),\n\u001b[32m      7\u001b[39m     learning_rate=trial.suggest_float(\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.2\u001b[39m, log=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mreg:squarederror\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m42\u001b[39m, n_jobs=\u001b[32m0\u001b[39m,\n\u001b[32m     16\u001b[39m )\n\u001b[32m     17\u001b[39m model = XGBRegressor(**params)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_te\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m pred = model.predict(X_test)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mean_absolute_error(y_te, pred)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1247\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\xgboost\\training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive - MSFT\\Poultary\\8 13 2025 GBDT\\.venv\\Lib\\site-packages\\xgboost\\core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# XGBoost study (GPU)\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "def xgb_objective(trial):\n",
        "    params = dict(\n",
        "        n_estimators=trial.suggest_int('n_estimators', 1500, 6000),\n",
        "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
        "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
        "        min_child_weight=trial.suggest_int('min_child_weight', 1, 8),\n",
        "        subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        reg_alpha=trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
        "        reg_lambda=trial.suggest_float('reg_lambda', 0.5, 5.0),\n",
        "        tree_method='gpu_hist', predictor='gpu_predictor', gpu_id=0,\n",
        "        objective='reg:squarederror', random_state=42, n_jobs=0,\n",
        "    )\n",
        "    model = XGBRegressor(**params)\n",
        "    model.fit(X_train, y_tr, eval_set=[(X_test, y_te)], verbose=False)\n",
        "    pred = model.predict(X_test)\n",
        "    return mean_absolute_error(y_te, pred)\n",
        "\n",
        "study_xgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42),\n",
        "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=10))\n",
        "study_xgb.optimize(xgb_objective, n_trials=40, n_jobs=1)\n",
        "print('XGB best MAE:', study_xgb.best_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CatBoost study (GPU)\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "def cat_objective(trial):\n",
        "    params = dict(\n",
        "        iterations=trial.suggest_int('iterations', 3000, 12000),\n",
        "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
        "        depth=trial.suggest_int('depth', 4, 10),\n",
        "        l2_leaf_reg=trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
        "        loss_function='MAE',\n",
        "        task_type='GPU', devices='0', random_seed=42,\n",
        "        od_type='Iter', od_wait=600, verbose=False,\n",
        "    )\n",
        "    model = CatBoostRegressor(**params)\n",
        "    model.fit(Pool(X_train, y_tr), eval_set=Pool(X_test, y_te), use_best_model=True, verbose=False)\n",
        "    pred = model.predict(X_test)\n",
        "    return mean_absolute_error(y_te, pred)\n",
        "\n",
        "study_cat = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42),\n",
        "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=10))\n",
        "study_cat.optimize(cat_objective, n_trials=30, n_jobs=1)\n",
        "print('CatBoost best MAE:', study_cat.best_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'p_cat' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m m_xgb = XGBRegressor(**p_xgb)\n\u001b[32m     16\u001b[39m m_xgb.fit(X_train, y_tr, eval_set=[(X_test, y_te)], verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m m_cat = CatBoostRegressor(**\u001b[43mp_cat\u001b[49m)\n\u001b[32m     19\u001b[39m m_cat.fit(Pool(X_train, y_tr), eval_set=Pool(X_test, y_te), use_best_model=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'p_cat' is not defined"
          ]
        }
      ],
      "source": [
        "# Retrain best models and evaluate, then save\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Best params\n",
        "p_lgb = study_lgb.best_params | dict(objective='regression', n_jobs=-1, verbosity=-1, device='gpu', gpu_platform_id=0, gpu_device_id=0)\n",
        "p_xgb = study_xgb.best_params | dict(tree_method='gpu_hist', predictor='gpu_predictor', gpu_id=0, objective='reg:squarederror', random_state=42, n_jobs=0)\n",
        "# p_cat = study_cat.best_params | dict(loss_function='MAE', task_type='GPU', devices='0', random_seed=42, verbose=False)\n",
        "\n",
        "# Retrain\n",
        "m_lgb = lgb.LGBMRegressor(**p_lgb)\n",
        "m_lgb.fit(X_train, y_tr, eval_set=[(X_test, y_te)], eval_metric='l1', callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False), lgb.log_evaluation(period=0)])\n",
        "\n",
        "m_xgb = XGBRegressor(**p_xgb)\n",
        "m_xgb.fit(X_train, y_tr, eval_set=[(X_test, y_te)], verbose=False)\n",
        "\n",
        "# m_cat = CatBoostRegressor(**p_cat)\n",
        "# m_cat.fit(Pool(X_train, y_tr), eval_set=Pool(X_test, y_te), use_best_model=True, verbose=False)\n",
        "\n",
        "# Evaluate\n",
        "import numpy as np\n",
        "\n",
        "def eval_model(model, X, y, name):\n",
        "    pred = model.predict(X)\n",
        "    mae = mean_absolute_error(y, pred)\n",
        "    mse = mean_squared_error(y, pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y, pred)\n",
        "    print(f\"{name} -> MAE: {mae:.6f}  MSE: {mse:.6f}  RMSE: {rmse:.6f}  R2: {r2:.6f}\")\n",
        "    return dict(model=name, MAE=mae, MSE=mse, RMSE=rmse, R2=r2)\n",
        "\n",
        "results = []\n",
        "results.append(eval_model(m_lgb, X_test, y_te, 'LGBM_Optuna'))\n",
        "results.append(eval_model(m_xgb, X_test, y_te, 'XGB_Optuna'))\n",
        "# results.append(eval_model(m_cat, X_test, y_te, 'CatBoost_Optuna'))\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LGBM_Optuna -> MAE: 0.062958  MSE: 0.008914  RMSE: 0.094414  R2: 0.877961\n",
            "XGB_Optuna -> MAE: 0.065131  MSE: 0.009187  RMSE: 0.095848  R2: 0.874226\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>MAE</th>\n",
              "      <th>MSE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LGBM_Optuna</td>\n",
              "      <td>0.062958</td>\n",
              "      <td>0.008914</td>\n",
              "      <td>0.094414</td>\n",
              "      <td>0.877961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>XGB_Optuna</td>\n",
              "      <td>0.065131</td>\n",
              "      <td>0.009187</td>\n",
              "      <td>0.095848</td>\n",
              "      <td>0.874226</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         model       MAE       MSE      RMSE        R2\n",
              "0  LGBM_Optuna  0.062958  0.008914  0.094414  0.877961\n",
              "1   XGB_Optuna  0.065131  0.009187  0.095848  0.874226"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def eval_model(model, X, y, name):\n",
        "    pred = model.predict(X)\n",
        "    mae = mean_absolute_error(y, pred)\n",
        "    mse = mean_squared_error(y, pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y, pred)\n",
        "    print(f\"{name} -> MAE: {mae:.6f}  MSE: {mse:.6f}  RMSE: {rmse:.6f}  R2: {r2:.6f}\")\n",
        "    return dict(model=name, MAE=mae, MSE=mse, RMSE=rmse, R2=r2)\n",
        "\n",
        "results = []\n",
        "results.append(eval_model(m_lgb, X_test, y_te, 'LGBM_Optuna'))\n",
        "results.append(eval_model(m_xgb, X_test, y_te, 'XGB_Optuna'))\n",
        "# results.append(eval_model(m_cat, X_test, y_te, 'CatBoost_Optuna'))\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to saved_models\\optuna_all_20250813_085609\n"
          ]
        }
      ],
      "source": [
        "# Save timestamped artifacts\n",
        "out_dir = os.path.join('saved_models', 'optuna_all_' + datetime.datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "joblib.dump(imputer, os.path.join(out_dir, 'imputer.joblib'))\n",
        "joblib.dump(scaler, os.path.join(out_dir, 'scaler.joblib'))\n",
        "joblib.dump(m_lgb, os.path.join(out_dir, 'lgbm_model.joblib'))\n",
        "joblib.dump(m_xgb, os.path.join(out_dir, 'xgb_model.joblib'))\n",
        "# from catboost import CatBoostRegressor\n",
        "# m_cat.save_model(os.path.join(out_dir, 'catboost_model.cbm'))\n",
        "\n",
        "with open(os.path.join(out_dir, 'metrics.json'), 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print('Saved to', out_dir)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
