{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GBDT weight regression on dataset_pruned.csv\n",
        "\n",
        "This notebook applies the paper's algorithm (see `#3_GBDT.txt`) to train Gradient Boosting Decision Trees on fused features (2D + 3D + C-ResNet50) from `dataset_pruned.csv`.\n",
        "\n",
        "- Split: 70% train / 30% test\n",
        "- Models: LightGBM and XGBoost\n",
        "- Hyperparameters: as specified in the paper\n",
        "- Preprocessing: feature normalization (improves performance per paper)\n",
        "- Metrics: MAE, MSE, RMSE, R2\n",
        "- Outputs: metrics and saved models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'base (Python 3.13.5)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Optional: silence warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DATA_PATH = 'dataset_pruned.csv'\n",
        "LABEL_COL = 'weight_kg'\n",
        "ID_COLS = ['Chicken_ID', 'Image_ID']\n",
        "\n",
        "assert os.path.exists(DATA_PATH), f\"Missing {DATA_PATH}\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f\"Loaded: {df.shape[0]} rows, {df.shape[1]} cols\")\n",
        "\n",
        "# Separate features/label\n",
        "y = df[LABEL_COL].values\n",
        "X = df.drop(columns=ID_COLS + [LABEL_COL])\n",
        "feature_names = X.columns.tolist()\n",
        "print(f\"Features: {len(feature_names)} (first 10): {feature_names[:10]}\")\n",
        "\n",
        "# Train/test split 70/30\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Normalization (paper notes it helps)\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Train/Test shapes:\", X_train_scaled.shape, X_test_scaled.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LightGBM and XGBoost per-paper hyperparameters\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "lgb_params = dict(\n",
        "    n_estimators=4000,\n",
        "    learning_rate=0.1,\n",
        "    num_leaves=15,\n",
        "    max_depth=5,\n",
        "    min_child_samples=15,\n",
        "    min_child_weight=0.01,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=1.0,\n",
        "    objective='regression',\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "xgb_params = dict(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    objective='reg:squarederror',  # modern alias for reg:linear\n",
        "    min_child_weight=1,\n",
        "    max_delta_step=0,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.7,\n",
        "    colsample_bylevel=1.0,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=1.0,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
        "xgb_model = XGBRegressor(**xgb_params)\n",
        "\n",
        "# Fit on normalized features (as per paper's normalization recommendation)\n",
        "lgb_model.fit(X_train_scaled, y_train)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Models trained.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, X_tr, y_tr, X_te, y_te, name):\n",
        "    pred_tr = model.predict(X_tr)\n",
        "    pred_te = model.predict(X_te)\n",
        "    mae = mean_absolute_error(y_te, pred_te)\n",
        "    mse = mean_squared_error(y_te, pred_te)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_te, pred_te)\n",
        "    print(f\"{name} -> MAE: {mae:.6f}  MSE: {mse:.6f}  RMSE: {rmse:.6f}  R2: {r2:.6f}\")\n",
        "    return dict(model=name, MAE=mae, MSE=mse, RMSE=rmse, R2=r2)\n",
        "\n",
        "results = []\n",
        "results.append(evaluate(lgb_model, X_train_scaled, y_train, X_test_scaled, y_test, 'LGBM'))\n",
        "results.append(evaluate(xgb_model, X_train_scaled, y_train, X_test_scaled, y_test, 'XGB'))\n",
        "\n",
        "pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save artifacts\n",
        "os.makedirs('artifacts', exist_ok=True)\n",
        "joblib.dump(scaler, 'artifacts/scaler.joblib')\n",
        "joblib.dump(lgb_model, 'artifacts/lgbm_model.joblib')\n",
        "joblib.dump(xgb_model, 'artifacts/xgb_model.joblib')\n",
        "\n",
        "# Save metrics\n",
        "metrics_path = 'artifacts/metrics.json'\n",
        "with open(metrics_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"Saved scaler/models/metrics to artifacts/\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
